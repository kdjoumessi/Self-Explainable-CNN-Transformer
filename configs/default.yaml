base:
  dataset: Fundus # in ['Fundus', AREDS]
  device: cuda 
  random_seed: 0
  test: True #True, False
  sample: 0 # number of samples to consider in the debugging mode

dset:
  save_path: null

data:
  binary: False 
  threshold: 2 # 1 for onset1 and 2 for unset2. to derive the binary metrics: used in the metrics file
  sampling_strategy: instance_balanced # instance_balanced / class_balanced / progressively_balanced. ref: https://arxiv.org/abs/1910.09217
  sampling_weights_decay_rate: 0.9 # if sampling_strategy is progressively_balanced, sampling weight will change from class_balanced to instance_balanced
  data_augmentation: # available operations are list in 'data_augmentation_args' below
    - random_crop
    - horizontal_flip
    - vertical_flip
    - color_distortion
    - rotation
    - translation

train:
  network: resnet50 # vit bagnet33 [33, 17, 9], resnet50 available networks are list in networks.yaml 
  res_baseline: False
  bag_baseline: False
  vit: False
  timm: False 
  vit_logit: False
  vit_large: False
  conv_cls: True #True
  train_with_att: False
  attention_and_fcl: False
  mhsa_conv: False  
  drsa: True 
  pretrained: true # load weights from pre-trained model training on ImageNet
  epochs: 70
  batch_size: 10 #8, 12, 16, 24 64
  num_workers: 24 # 0, 2, 4 good = 2 number of cpus used to load data at each step
  criterion: cross_entropy # [cross_entropy, mean_square_error ] available criterions are list in 'criterion_args' below
  loss_weight: null # null / balance / dynamic / list with shape num_classes. Weights for loss function. Don't use it with weighted sampling!
  loss_weight_decay_rate: 0 # if loss_weights is dynamic, loss weight will decay from balance to equivalent weights
  warmup_epochs: 0 # set to 0 to disable warmup
  kappa_prior: false # save model with higher kappa or higher accuracy in validation set
  save_interval: 5 # the epoch interval of saving model
  eval_interval: 1 # the epoch interval of evaluating model on val dataset
  sample_view: false # save and visualize a batch of images on Tensorboard
  sample_view_interval: 100 # the steps interval of saving samples on Tensorboard. Note that frequently saving images will slow down the training speed.
  pin_memory: true # enables fast data transfer to CUDA-enabled GPUs
  lambda_l1: 0.000008
  lambda_l2: 0 #  .000004 
  
fct_mhsa:  
  ffn: True
  gdfn: False   # Gated-Dconv Feed-Forward Network 
  drsa: Fase
  n_head: 8
  mhsa_conv: True
  downsample_factor: 2
  class_from_att: True
  attention_and_fcl: False
  
drsa:
  num_head: 1  
  lr_dim: 30 # bagnet: 60/2=30, resnet: 16/2=8 
  maxpool: True
  is_mhsa: False # if False set the number of head to 1
  conv_drsa: True #True
  with_drsa: True
  gdfn_layer: True
  head_size: 10 #bag=10, res=8 #res = 32, 16, 8, 4, bagnet=8, 10, 20, 360, 3600
  ld_head_size: 10 #10 #bag=10, res=8, lr_bag= 900, 10, 5
  window_size: None #[10,10] #10
  reduction_factor: 2
  downsample_factor: [2, 4]
  baseline_conv_drsa: None #True
    
solver:
  optimizer: SGD # SGD / ADAM
  learning_rate: 0.001 # initial learning rate
  lr_scheduler: clipped_cosine # [cosine, clipped_cosine] available schedulers are list in 'scheduler_args' below. please remember to update scheduler_args when number of epoch changes.
  momentum: 0.9 # only for SGD. set to 0 to disable momentum
  nesterov: true # only for SGD.
  weight_decay: 0.0005 # set to 0 to disable weight decay

criterion_args:
  cross_entropy: {}
  mean_square_error: {}
  mean_absolute_error: {}
  smooth_L1: {}
  kappa_loss:
    num_classes: 2 # [2, 5]
  focal_loss:
    alpha: 5
    reduction: mean

# please refer to documents of torch.optim
scheduler_args:
  exponential:
    gamma: 0.6 # multiplicative factor of learning rate decay
  multiple_steps:
    milestones: [15, 25, 45]
    gamma: 0.1 # multiplicative factor of learning rate decay
  cosine:
    T_max: 50 # maximum number of iterations
    eta_min: 0 # minimum learning rate
  reduce_on_plateau:
    mode: min
    factor: 0.1 # new learning rate = factor * learning rate
    patience: 5 # number of epochs with no improvement after which learning rate will be reduced.
    threshold: 0.0001 # threshold for measuring the new optimum
    eps: 0.00001 # minimal decay applied to learning rate
  clipped_cosine:
    T_max: 50
    min_lr: 0.0001 #

data_augmentation_args:
  horizontal_flip:
    prob: 0.5
  vertical_flip:
    prob: 0.5
  color_distortion:
    prob: 0.5
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
  random_crop: # randomly crop and resize to input_size
    prob: 0.5
    scale: [0.87, 1.15] # range of size of the origin size cropped
    ratio: [0.7, 1.3] # range of aspect ratio of the origin aspect ratio cropped
  rotation:
    prob: 0.5
    degrees: [-180, 180]
  translation:
    prob: 0.5
    range: [0.2, 0.2]
  grayscale: # randomly convert image to grayscale
    prob: 0.5
  gaussian_blur: # only available for torch version >= 1.7.1.
    prob: 0.2
    kernel_size: 7
    sigma: 0.5
  value_fill: 0 # NOT a data augmentation operation. pixel fill value for the area outside the image
